# **TARP REVIEW - 1**

## Topic - Chatbot System for Cross Lingual Communication using NLP

#### **Team Members:**

S.Tarrun - 20BCE2434 

Parth S Parker - 20BCB0061

#### **Submitted To:** Prof. Manoov R

# 




### **Product Vision**

Our product vision is to create an NLP induced chatbot system that will revolutionize cross-lingual communication. Our chatbot will use natural language processing (NLP) algorithms to understand and interpret different languages and provide seamless communication between people who speak different languages.

The chatbot will be designed to support multiple languages, and will use machine learning algorithms to improve its language understanding over time. This will enable users to communicate with each other in their native language, without the need for human translators or interpretation services.

Our chatbot will be intuitive and user-friendly, and will work across multiple platforms including web, mobile and desktop applications. It will provide a simple and easy-to-use interface that users can access from anywhere in the world.

We envision that our NLP induced chatbot system will have a significant impact on businesses, governments, and individuals around the world. It will enable businesses to expand their customer base by providing support in multiple languages, and will help governments to communicate effectively with people who speak different languages.

Ultimately, our chatbot system will break down language barriers and bring people together, creating a more connected and inclusive world.


### **Problem Description**


1. **Cross-lingual communication** is a significant challenge that affects many individuals and organizations worldwide. Language barriers can cause confusion, misinterpretation, and ultimately hinder effective communication. Traditional solutions to language barriers, such as human translators or interpretation services, can be costly, time-consuming, and not always readily available.

2. Additionally, **language barriers** can also create obstacles for businesses and organizations looking to expand their customer base or enter new markets. Providing support in multiple languages can be a daunting and costly task, especially for small to medium-sized enterprises.

3. Furthermore, language barriers can also create challenges for governments communicating with people who speak different languages, potentially impacting crucial services such as emergency response, public health, or legal matters.

4. Overall, language barriers are a significant problem that affects communication across different industries and sectors. This challenge highlights the need for innovative solutions that can facilitate cross-lingual communication more efficiently, cost-effectively, and with increased accuracy. Our NLP Induced Chatbot System for Cross Lingual Communication aims to address these challenges by providing a cutting-edge, user-friendly, and cost-effective solution to facilitate communication between individuals who speak different languages.

5. Customer dissatisfaction: In customer service interactions, language barriers and inability to read can lead to dissatisfaction among customers who may not be able to effectively communicate their needs or understand information provided by the service providers. This can impact customer satisfaction, loyalty, and the reputation of businesses

### **Motivation for Topic Selection:**

The selection of NLP Induced Chatbot System for Cross Lingual Communication as a topic was motivated by several factors. One of the most significant factors is the increasing need for cross-lingual communication in today's globalized world. The ability to communicate effectively across different languages is crucial for individuals, businesses, and governments to succeed in a diverse and interconnected world.

**-** Increasing need for cross-lingual communication in today's globalized world.

**-** Recent advancements in NLP technology and machine learning have made it possible to develop chatbots that can interpret and understand multiple languages, making cross-lingual communication more accessible and efficient.

**-** The potential of chatbot technology to transform the way we communicate and conduct business globally, providing a cost-effective and scalable solution to language barriers.

**-** Potential social impact of chatbot technology in promoting inclusivity, social cohesion, and overcoming discrimination and prejudice based on language barriers.

### **Literature Review**

#### **1. Paper Title: "Multilingual Natural Language Processing: A Survey"**
**Introduction:** This literature review provides an overview of multilingual natural language processing (NLP) techniques, which involve processing text in multiple     languages. It discusses various approaches such as machine translation, cross-lingual embeddings, and multilingual representations, along with their applications in    tasks such as sentiment analysis, named entity recognition, and machine translation.
**Major Drawbacks:**

Limited coverage of recent advancements in multilingual NLP, as the field evolves rapidly.
Limited discussion on challenges and limitations of multilingual NLP, such as handling code-switching or dialects in multilingual text.
Potential bias towards major languages, with limited coverage of NLP in low-resource or underrepresented languages.

#### **2. Paper Title: "Cross-lingual Text Classification: A Comprehensive Survey"**

**Introduction:** This literature review focuses on cross-lingual text classification, which involves training a text classification model in one language and applying it to another language. It discusses various approaches such as transfer learning, domain adaptation, and cross-lingual embeddings, and reviews their strengths and weaknesses in cross-lingual text classification tasks.
**Major Drawbacks:**

Limited focus on other cross-lingual NLP tasks beyond text classification, such as named entity recognition or sentiment analysis.
Limited coverage of recent advancements in cross-lingual text classification, as the field continues to evolve rapidly.
Limited discussion on challenges and limitations of cross-lingual text classification, such as handling language-specific features or differences in text representations.

#### **3. Paper Title: "Language Modeling for Code-Switching: A Survey"**

**Introduction:** This literature review focuses on language modeling for code-switching, which involves modeling text that contains a mix of two or more languages. It discusses various approaches such as rule-based, statistical, and deep learning-based methods, and reviews their applications in code-switching tasks, challenges, and limitations.
**Major Drawbacks:**

Limited focus on other code-switching related tasks, such as sentiment analysis, named entity recognition, or machine translation.
Limited coverage of recent advancements in language modeling for code-switching, as the field is relatively new and rapidly evolving.
Potential bias towards specific languages or code-switching scenarios, with limited coverage of other languages or regions.

#### ** 4.Paper Title: "Survey on Neural Network Approaches for Natural Language Processing"**
**Introduction**: This literature review provides an overview of various neural network approaches used in natural language processing (NLP), including recurrent neural networks (RNNs), convolutional neural networks (CNNs), and transformer models. It discusses their applications in tasks such as machine translation, sentiment analysis, and text generation.
**Major Drawbacks:**

Limited focus on recent advancements in NLP, as the field evolves rapidly.
Lack of in-depth analysis and comparison of different neural network approaches.
Limited discussion on challenges and limitations of neural network-based NLP models.

#### **5. Paper Title: "A Survey on Machine Translation: Current Techniques and Future Perspectives"**
**Introduction:** This literature review provides a comprehensive survey of machine translation techniques, including rule-based, statistical, and neural machine translation (NMT) approaches. It discusses the strengths and weaknesses of each approach and explores future perspectives in machine translation research.
**Major Drawbacks:**

Potential bias towards older techniques, as the field of machine translation has evolved significantly in recent years with the rise of NMT.
Limited focus on domain-specific machine translation or low-resource language pairs.
Limited discussion on challenges and limitations of machine translation, such as handling rare or out-of-vocabulary words.

#### **6. Paper Title: "Sentiment Analysis: A Survey of Techniques and Applications"**
Introduction: This literature review provides an overview of sentiment analysis techniques, which involve NLP methods for identifying sentiment or emotion in text data. It discusses various approaches such as lexicon-based, machine learning-based, and deep learning-based methods, along with their applications in sentiment analysis tasks.
Major Drawbacks:

Limited coverage of emerging techniques in sentiment analysis, such as transfer learning or domain adaptation.
Limited discussion on challenges and limitations of sentiment analysis, such as handling sarcasm or irony in text.
Potential bias towards English-centric sentiment analysis, with limited coverage of sentiment analysis in other languages.

#### **7. Paper Title: "Text Generation Techniques: A Comprehensive Survey"**
Introduction: This literature review provides a comprehensive survey of text generation techniques, including rule-based, statistical, and deep learning-based approaches. It discusses their applications in tasks such as text summarization, machine translation, and dialogue generation, and compares their strengths and weaknesses.
**Major Drawbacks:**

Limited focus on recent advancements in text generation, such as transformer-based models or reinforcement learning approaches.
Limited discussion on challenges and limitations of text generation, such as controlling for quality, coherence, and diversity in generated text.
Potential bias towards specific text generation tasks, with limited coverage of text generation in other domains or applications.

#### **8. Paper Title: "Challenges and Limitations of NLP for Low-Resource Languages"**
Introduction: This literature review focuses on the challenges and limitations of NLP techniques for low-resource languages, which have limited data or resources for building NLP models. It discusses issues such as data scarcity, lack of annotated data, and resource constraints, and reviews existing approaches to address these challenges.
**Major Drawbacks:**

Limited coverage of recent advancements in NLP for low-resource languages, as the field continues to evolve rapidly.
Limited focus on specific low-resource languages or regions, with potential bias towards certain languages.
Limited discussion on potential ethical considerations, such as potential bias or fairness issues in NLP models for low-resource languages.

#### **9.Paper Title: "Neural Machine Translation: A Review"**

Introduction: This literature review provides an overview of neural machine translation (NMT), which is a popular approach in NLP for automatic translation between languages using neural networks. It discusses various NMT architectures, such as encoder-decoder models, attention mechanisms, and transformer models, along with their strengths and limitations in terms of translation quality, training data requirements, and computational complexity.
**Major Drawbacks:**

Limited coverage of other traditional machine translation approaches, such as statistical or rule-based methods, which may still be relevant in certain scenarios.
Potential bias towards major languages, with limited coverage of NMT in low-resource or underrepresented languages.
Limited discussion on challenges and limitations of NMT, such as handling rare or out-of-vocabulary words, domain adaptation, or translating idiomatic expressions.

#### **10. Paper Title: "Unsupervised Neural Machine Translation: A Survey"**

**Introduction:** This literature review focuses on unsupervised neural machine translation, which aims to train translation models without parallel training data, making it suitable for low-resource language pairs. It discusses various approaches, such as unsupervised domain adaptation, self-supervised learning, and multilingual NMT, along with their strengths and limitations in terms of translation quality, data requirements, and model robustness.
**Major Drawbacks:**

Limited coverage of recent advancements in unsupervised NMT, as the field is still rapidly evolving with ongoing research.
Limited focus on other unsupervised machine translation techniques beyond neural approaches, such as phrase-based or pivot-based methods.
Limited discussion on challenges and limitations of unsupervised NMT, such as handling language-specific syntactic or semantic differences, or mitigating the risk of generating inaccurate translations without parallel data for supervision.

### ** Technology Required**

**Natural Language Processing (NLP):** NLP techniques are used to analyze and process text or voice inputs in different languages, including tasks such as language identification, text segmentation, named entity recognition, part-of-speech tagging, sentiment analysis, and machine translation.

**Machine Translation (MT):** MT technology is used to automatically translate text from one language to another. This could involve rule-based, statistical, or neural machine translation approaches, depending on the requirements and accuracy desired.

**Speech Recognition:** Speech recognition technology is used to convert voice recordings into text, allowing users to provide input through voice recordings. This may involve automatic speech recognition (ASR) techniques that transcribe spoken language into written text.

**Text-to-Speech (TTS):** TTS technology is used to convert text into spoken voice output. This could involve various TTS techniques, including concatenative TTS, formant synthesis, or neural TTS, depending on the desired voice quality and naturalness.

**Image Processing:** Image processing techniques may be used to process images of text inputs, such as when users take a photo of a document or a signboard. Optical character recognition (OCR) technology could be used to extract text from the image for further processing.

**Backend Technologies:** Backend technologies such as server-side programming languages (e.g., Python, Node.js), web frameworks (e.g., Flask, Express), and databases (e.g., MySQL, MongoDB) may be used to handle the processing, storage, and retrieval of user inputs and outputs, as well as the integration with other systems.

**APIs and Libraries:** APIs and libraries for NLP, MT, ASR, TTS, and OCR, such as Google Cloud Translation API, Microsoft Translator API, CMU Sphinx, Google Cloud Speech-to-Text API, Google Cloud Text-to-Speech API, and Tesseract OCR, may be used to leverage existing tools and technologies for specific functionalities.

**Development Tools:** Development tools such as integrated development environments (IDEs), version control systems (e.g., Git), project management tools, and collaboration platforms may be used to facilitate the development, testing, and deployment of the chatbot system.

**Deployment Technologies:** Deployment technologies such as cloud platforms (e.g., AWS, Google Cloud, Azure) or on-premises servers may be used to host and deploy the chatbot system, depending on the chosen infrastructure and deployment strategy.


### **Technology Gap compared to other compatitors**

**-** In terms of the technology gap with the current generation, the main difference is the use of AI and machine learning to implement chatbots. 

1. **Accuracy:** One of the critical challenges of NLP-based chatbot systems is accuracy in interpreting and understanding different languages. Current systems still struggle with accurately interpreting and understanding the nuances of language, leading to incorrect translations and misunderstandings.

2. **Contextual understanding:** Understanding the context in which a conversation is taking place is crucial for effective communication, especially when dealing with multiple languages. Current chatbot systems struggle to understand the context and often provide generic responses that are not relevant to the conversation.

3. **Integration:** Integrating NLP-based chatbot systems into existing communication channels, such as web, mobile, or desktop applications, can be challenging. Existing systems may require significant customization to integrate seamlessly into these channels.

4. **Scalability:** As the number of languages and users increases, scalability can become an issue for NLP-based chatbot systems. Current platforms may struggle to handle large volumes of traffic and data, leading to slow response times and decreased accuracy.

5. **Multimodal Communication:** Most existing chatbot systems only support text-based communication, but for cross-lingual communication, it may be necessary to support voice and video communication.

6. **Translated Speech:** The most feature we are bring in is the ability of the chat bot to read out the message in the user understandable language. 

### **Currently existing platforms:**

**1. Google Translate:** Google Translate is a widely used language translation service that offers translation capabilities for text, images, and voice inputs. It supports a wide range of languages and provides voice synthesis for translated text in multiple languages.

**2. Microsoft Translator:** Microsoft Translator is another popular language translation service that offers translation for text, images, and voice inputs. It supports various languages and provides voice synthesis for translated text.

**3. Amazon Polly:** Amazon Polly is a cloud-based text-to-speech (TTS) service that provides voice synthesis capabilities in multiple languages. It can convert text into natural-sounding speech with different voice styles and accents.

**4.IBM Watson Language Translator:** IBM Watson Language Translator is a language translation service that provides text translation capabilities for multiple languages. It also supports voice synthesis for translated text.

**5.DeepL:** DeepL is an AI-powered language translation service that offers translation capabilities for text and provides voice synthesis for translated text in multiple languages.

**6.Dialogflow:** Dialogflow is a conversational AI platform by Google that allows building chatbots and virtual assistants. It provides NLP capabilities for understanding user inputs in multiple languages and supports text-to-speech (TTS) for generating voice responses.

**7.BotStar:** BotStar is a chatbot development platform that offers NLP-based language processing capabilities for understanding user inputs in multiple languages. It supports voice synthesis for generating voice responses in different languages.

### **Project work flow**

#### **Requirement Analysis:**
We worked together to understand the project requirements and gather detailed specifications from stakeholders.
We analyze the language translation, voice synthesis, image processing, and other functional requirements of the chatbot.
We also consider non-functional requirements such as performance, scalability, and security.

#### **Design and Architecture:**

We collaborate to design the architecture of the chatbot system, including the data flow, APIs, databases, and integration points.
We decide on the technologies, APIs, and libraries to be used for different components of the chatbot system.
We create a design document, including the system architecture, data models, and flowcharts.

#### **Development:**

Tarrun will take responsibility for the NLP and language translation components, implementing the necessary NLP techniques, machine translation models, and APIs.
Parth takes responsibility for the speech recognition and text-to-speech components, implementing the ASR and TTS technologies, and integrating them with the chatbot system.
We collaborate to develop the image processing component, implementing OCR techniques to extract text from images, and integrating it with the chatbot system.
They follow an agile development methodology, with regular sprints, code reviews, and testing to ensure quality and progress.

#### **Testing and Quality Assurance:**

We collaborate to conduct extensive testing of the chatbot system, including unit testing, integration testing, and functional testing.
We also perform performance testing, security testing, and user acceptance testing to ensure that the system meets the required quality standards.
We identify and fix any bugs or issues discovered during testing and ensure that the chatbot system is reliable and robust.

#### **Deployment and Integration:**

We work together to deploy the chatbot system on the chosen infrastructure, such as a cloud platform or on-premises servers.
we integrate the chatbot system with other required components, such as databases, APIs, and external systems, to ensure smooth functioning and data flow.
we thoroughly test the integrated system and ensure that it is ready for production use.

#### **Launch and Monitoring:**
We collaborate to launch the chatbot system, making it available to users for input through text, voice, or image inputs.
We monitor the system for any issues or performance bottlenecks and take proactive measures to resolve them.
We gather feedback from users and stakeholders, and continuously improve the system based on feedback and usage patterns.

#### **Documentation and Knowledge Sharing:**

We document the entire development process, including the architecture, design decisions, implementation details, and testing results.
We create user manuals, technical documentation, and knowledge base resources to help users understand and use the chatbot system effectively.
We also conduct knowledge sharing sessions with team members or stakeholders to transfer knowledge and expertise about the chatbot system.

#### **Progress Reporting and Communication:**

We regularly update progress on their respective responsibilities and share it with each other and other stakeholders.
We hold regular meetings to discuss progress, challenges, and next steps in the development process.
We use project management tools, communication channels, and other means to ensure effective communication and coordination throughout the development process.

### **Phase Based Work Break Structure:**

We are following incremental model as it is best suited for project. It reduces the room for error and consumes less time.
Our project follows as mentioned below.
 
![image](https://user-images.githubusercontent.com/124272210/230575453-e10b3d04-3265-453e-ba8b-09d9acb093e2.png)


### **Responsibility Based Work Break-down Structure:**

### **Tarrun:**

#### **Project Management**

Define project scope, objectives, and timeline
Coordinate team members and assign tasks
Monitor project progress and ensure adherence to timelines
Communicate with stakeholders and manage project risks

#### **Research and Requirements Gathering**

Conduct research on NLP-based chatbot technologies, libraries, and APIs
Gather requirements from stakeholders, including text translation, voice recognition, and voice synthesis capabilities
Document and finalize the project requirements

#### **Design and Architecture**
Design the overall system architecture, including input processing, language translation, voice recognition, and voice synthesis components
Design data models, algorithms, and APIs for each component
Define integration points and communication protocols between components
Collaborate with Parth on user interface design and provide input on integrating front-end and back-end components

#### **Front-end Development**

Develop the user interface for the chatbot, including input options for text, image, and voice inputs
Implement input processing logic to handle different input types, such as text extraction from images or voice-to-text conversion
Implement user prompts and responses for language selection, input confirmation, and error handling
Collaborate with Parth on integrating front-end and back-end components and resolving any interface-related issues

#### **Testing and Quality Assurance**

Develop and execute test cases to validate the functionality and performance of the chatbot
Conduct integration testing to ensure seamless communication between front-end and back-end components
Perform user acceptance testing to gather feedback from stakeholders
Collaborate with Parth on identifying and fixing defects, performing code reviews, and ensuring code quality and adherence to coding standards

### **Parth:**

#### **Back-end Development**
Implement APIs and services for language translation using NLP techniques, such as neural machine translation or rule-based translation
Implement APIs and services for voice recognition to convert voice inputs into text
Implement APIs and services for voice synthesis to convert translated text into spoken voice
Implement error handling, logging, and monitoring for the back-end services

#### **Deployment and Deployment**

Prepare the chatbot for deployment to production environment
Deploy the chatbot to a suitable hosting environment, such as a cloud server or a containerized environment
Configure and optimize the chatbot for performance, security, and scalability
Set up monitoring and logging for production environment
Prepare documentation for deployment, maintenance, and troubleshooting

#### **Maintenance and Support**

Provide ongoing maintenance and support for the chatbot, including bug fixes, performance optimizations, and updates to language translation models or voice synthesis libraries
Monitor system performance and troubleshoot production issues
Provide support to end-users and address their issues or concerns
Collaborate with Tarrun to gather feedback from stakeholders and continuously improve the chatbot's functionality and user experience
 
![image](https://user-images.githubusercontent.com/124272210/230576932-194650f3-d727-4034-9249-694376fd3552.png)



# **TARP REVIEW - 2**

## **Ideation and Conversations**

Parth: Hey Tarrun, I've been thinking about developing an NLP-induced chatbot system for cross-lingual communication. What do you think?

Tarrun: That sounds like a great idea. What do you envision this chatbot system to do?

Parth: I envision it as a platform that can interpret and understand multiple languages, and provide real-time translation for text, voice, and video communication. The system should also be able to understand the context of the conversation and provide accurate and relevant responses.

Tarrun: That sounds like a challenging project, but also very promising. What are the technical requirements for such a system?

Parth: Well, we will need to use advanced NLP algorithms and machine learning techniques to enable the chatbot to understand and interpret different languages accurately. We'll also need to develop an intuitive user interface that allows users to communicate seamlessly across different languages.

Tarrun: And what about the limitations of such a system?

Parth: One of the significant limitations would be the accuracy of the translation. Even the most advanced NLP algorithms have a limited understanding of language nuances and can make mistakes in translation. The chatbot system may also face challenges in handling low-resource languages or dialects with limited training data.

Tarrun: What about the business plan? How do you envision this system generating revenue?

Parth: I think we could monetize the system by offering subscription-based services to businesses that operate in multiple countries and require cross-lingual communication. We could also offer the system as a standalone product for individual users who require cross-lingual communication for personal use.

Tarrun: That sounds like a good plan. And what about the future scope of this system?

Parth: I think there is enormous potential for this system to expand into other areas such as language learning, customer support, and e-commerce. The chatbot could also be integrated with other existing communication channels, such as social media platforms, messaging apps, and email.

Tarrun: It sounds like a comprehensive plan. How do you see us building the team for this project?

Parth: We'll need to hire NLP experts, machine learning engineers, UI/UX designers, and software developers to build this system. We could also consider partnering with academic institutions or research labs to tap into the latest NLP research.

Tarrun: It seems like a challenging project, but I think it has great potential. Let's start working on the project plan and put together a team.

To begin with, we conducted extensive market research to understand the current landscape of cross-lingual communication and the potential demand for their chatbot system. We found that businesses with a global presence, particularly in industries such as finance, healthcare, and tourism, would benefit greatly from such a system. Additionally, there was a growing need for cross-lingual communication in personal contexts, such as online dating and language exchange.

Based on their research, we  developed a detailed business plan, outlining the product's features, target markets, and revenue streams. We also identified potential competitors and ways to differentiate their chatbot system. They decided to focus on the system's accuracy, speed, and ability to handle multiple languages simultaneously.

The technical requirements of the chatbot system were also challenging. We realized that they would need to invest heavily in NLP algorithms and machine learning models to achieve the level of accuracy they desired. We also knew that designing an intuitive user interface that could handle multiple languages would be crucial to the system's success.

Assembling a team with the necessary expertise was a significant challenge. We knew that we would need to hire top talent in NLP, machine learning, and UI/UX.

### Obsidian



We used obsidian to save and keep track of our progress. Obsidian is a powerful note-taking application that can be used to facilitate ideation and conversation for implementing an NLP Induced Chatbot System for Cross Lingual Communication. Here are some ways Obsidian can be used in this context:

**Brainstorming Ideas:**
Obsidian can be used to capture and organize ideas during brainstorming sessions. The application provides a flexible and customizable platform for creating and linking notes. Participants in the ideation and conversation process can use Obsidian to jot down their thoughts and ideas, and link them to related concepts. This can help to generate new insights and identify potential solutions for the NLP Induced Chatbot System.

**Collaborative Note-Taking:**
Obsidian supports real-time collaboration, which makes it ideal for group discussions and meetings. Participants can share their notes and ideas with each other, and make changes or additions in real-time. This can help to foster a collaborative and inclusive environment, where everyone's contributions are valued and incorporated into the final product.

**Knowledge Management:**
Obsidian can be used to manage knowledge and information related to the NLP Induced Chatbot System. This includes documenting technical requirements, design specifications, and project timelines. The application provides a powerful search function that makes it easy to find and retrieve information quickly. This can help to ensure that the project stays on track and that everyone involved has access to the information they need.

**Task Management:**
Obsidian can be used to manage tasks and assignments related to the NLP Induced Chatbot System. Participants can create to-do lists, assign tasks to team members, and track progress over time. This can help to ensure that the project stays on schedule and that everyone is accountable for their contributions.

In summary, Obsidian can be a valuable tool for facilitating ideation and conversation for implementing an NLP Induced Chatbot System for Cross Lingual Communication. It can be used for brainstorming ideas, collaborative note-taking, knowledge management, and task management. By using Obsidian in conjunction with other tools and methodologies, such as NLP tools, machine learning algorithms, cloud computing, APIs, and programming languages, teams can ensure that the project is executed efficiently and effectively.

![image](https://user-images.githubusercontent.com/124272210/230565676-bfaefaee-7fe3-443e-b5c5-77dc0307f425.png)



### **Methodology and Requirements**

To implement a NLP induced chatbot system for cross-lingual communication, the following technical requirements and methodology can be followed:

1. **Natural Language Processing (NLP) Tools:**
The first technical requirement is to have a strong foundation in NLP tools such as sentiment analysis, named entity recognition, part-of-speech tagging, and language translation. These tools help to analyze and understand the user's input, generate appropriate responses, and translate them into different languages.

2. **Machine Learning Algorithms:**
Machine learning algorithms are crucial for training the NLP models used in the chatbot system. Algorithms such as neural networks, decision trees, and support vector machines can be used to train models that can accurately interpret and translate user inputs.

3. **Cloud Computing:**
Cloud computing is essential for deploying the chatbot system on a scalable infrastructure. Cloud platforms such as AWS, Google Cloud, and Microsoft Azure provide a cost-effective and flexible solution for hosting and managing the chatbot system.

4. **APIs:**
Application programming interfaces (APIs) are essential for integrating the NLP tools and machine learning algorithms into the chatbot system. APIs such as Google Translate, Amazon Comprehend, and Microsoft Text Analytics can be used to perform the necessary NLP tasks.

5. **Programming Languages:**
Programming languages such as Python, Java, and JavaScript are commonly used for developing chatbot systems. These languages provide robust libraries and frameworks for NLP, machine learning, and web development.

### Methodology:

1. **Data Collection:**
The first step in implementing an NLP-induced chatbot system for cross-lingual communication is to collect data in multiple languages. The data can come from various sources such as social media, news articles, or user-generated content.

2. **Data Preprocessing:**
After collecting the data, the next step is to preprocess and clean it. This involves removing any noise or irrelevant information, tokenizing the text, and performing any necessary normalization.

3. **NLP Model Training:**
Once the data is preprocessed, the NLP models such as sentiment analysis, named entity recognition, part-of-speech tagging, and language translation can be trained on the data. The models should be trained on a wide variety of languages to ensure that they can accurately interpret and translate each one.

4. **Chatbot Development:**
After training the NLP models, the next step is to develop the chatbot system. This involves integrating the NLP models into a conversational interface that can interpret user input and generate appropriate responses.

5. **User Interface Design:**
A user-friendly interface is crucial to ensure that users can communicate seamlessly across different languages. The user interface should be designed to handle multiple languages and provide clear instructions to the user.

6. **Deployment:**
After developing the chatbot system, it is deployed on a scalable infrastructure such as cloud computing platforms. The system should be thoroughly tested to ensure that it meets the desired level of accuracy and performance.

7. **Monitoring and Maintenance:**
Once the chatbot system is deployed, it is essential to continue monitoring its performance to ensure that it remains accurate and up-to-date with the latest NLP advancements. Maintenance tasks such as updating NLP models, improving the user interface, and fixing bugs should also be performed regularly.

In conclusion, implementing an NLP-induced chatbot system for cross-lingual communication requires a strong foundation in NLP tools, machine learning algorithms, cloud computing, APIs, and programming languages. The methodology involves data collection, preprocessing, NLP model training, chatbot development, user interface design, deployment, and monitoring and maintenance. A successful implementation requires a multidisciplinary team with expertise in NLP, machine learning, UI/UX, and software development.

# **TARP REVIEW - 3**

### **Video Recording**
https://drive.google.com/file/d/1NcUaZPfkpaBw0PNeFmjOPKFtdVR9gDAG/view?usp=share_link
### **Code**
```pip install pytesseract # For recognising and extracting the text from the image ```<br/>
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting pytesseract
  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)
Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from pytesseract) (8.4.0)
Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.9/dist-packages (from pytesseract) (23.0)
Installing collected packages: pytesseract
Successfully installed pytesseract-0.3.10 <br/>
```!sudo apt install tesseract-ocr #OCR developed by google it can sence 100 languages```<br/>
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  tesseract-ocr-eng tesseract-ocr-osd
The following NEW packages will be installed:
  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd
0 upgraded, 3 newly installed, 0 to remove and 24 not upgraded.
Need to get 4,850 kB of archives.
After this operation, 16.3 MB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1 [1,598 kB]
Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1 [2,990 kB]
Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr amd64 4.1.1-2build2 [262 kB]
Fetched 4,850 kB in 2s (1,981 kB/s)
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)
debconf: falling back to frontend: Readline
debconf: unable to initialize frontend: Readline
debconf: (This frontend requires a controlling tty.)
debconf: falling back to frontend: Teletype
dpkg-preconfigure: unable to re-open stdin: 
Selecting previously unselected package tesseract-ocr-eng.
(Reading database ... 122349 files and directories currently installed.)
Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1_all.deb ...
Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1) ...
Selecting previously unselected package tesseract-ocr-osd.
Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1_all.deb ...
Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1) ...
Selecting previously unselected package tesseract-ocr.
Preparing to unpack .../tesseract-ocr_4.1.1-2build2_amd64.deb ...
Unpacking tesseract-ocr (4.1.1-2build2) ...
Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1) ...
Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1) ...
Setting up tesseract-ocr (4.1.1-2build2) ...
Processing triggers for man-db (2.9.1-1) ...<br/>
```
import pytesseract
import shutil #Provides high level interfacing 
import os  # provides functions for interacting with the operating system 
import random # Generates random numbers 
import cv2 # For computer vision functionalities for image and video processing.
import numpy as np #provides support for working with arrays and matrices of numerical data
import matplotlib.pyplot as plt # used for creating visualizations and plots
from PIL import Image
```
### **Extracting the text from the image**
```ori_image = cv2.imread("/content/hqdefault.jpg") #Extracting the image 
ori_img = cv2.cvtColor(ori_image, cv2.COLOR_BGR2RGB) # Appyling color
plt.imshow(ori_img) 
plt.axis('off')
plt.show()
```
![image](https://user-images.githubusercontent.com/110376310/232060004-f5ab5370-6cb7-478a-be23-f27242fdb47b.png)
```
fixed_img = cv2.resize(ori_img, None, fx=1.2, fy=1.2, interpolation=cv2.INTER_CUBIC)
plt.imshow(fixed_img)
plt.axis("off")
```
![image](https://user-images.githubusercontent.com/110376310/232060072-fcf788b3-cb0b-42e1-b953-119598ceb9e0.png)
```
ogimg = cv2.cvtColor(fixed_img, cv2.COLOR_RGB2GRAY) #to convert an RGB image to gray scale 
```
```
kernel = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3)) # library to create a structuring element for morphological operations.
dilation = cv2.dilate(ogimg, kernel, iterations=1) #applying dilation 
plt.imshow(dilation)
plt.axis("off")
```
![image](https://user-images.githubusercontent.com/110376310/232060165-4f924598-d5f4-402c-81bc-a74911a973c0.png)
```
kernel = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3))
erosion= cv2.erode(dilation, kernel, iterations=1) # erostion 
plt.imshow(erosion)
plt.axis("off")
```
![image](https://user-images.githubusercontent.com/110376310/232060549-45e0633c-8c0a-4892-9a05-0386a12c645f.png)
```
binary = cv2.threshold(cv2.medianBlur(erosion, 3), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1] # thresholding for converting the image to bianry
plt.imshow(binary)
plt.axis("off")
```
![image](https://user-images.githubusercontent.com/110376310/232060648-d54016de-a1d2-4507-a46a-c48ba313c49c.png)
```
gsbin = binary.astype(np.uint8) #storing the binary image in 8 bit
# gsbin= 255 * gsbin
plt.imshow(gsbin)
```
![image](https://user-images.githubusercontent.com/110376310/232060685-b9aa6e4d-4e88-41e1-ad8e-27fa8aa2964b.png)
```
image2 = cv2.cvtColor(255-gsbin, cv2.COLOR_GRAY2RGB) #to convert a grayscale image represented by the 255-gsbin variable to an RGB image.
plt.imshow(image2)
plt.axis('off')
plt.show()
```
![image](https://user-images.githubusercontent.com/110376310/232060736-53daae57-a82f-420e-928c-97f61b3f10ec.png)
```
import matplotlib.image
matplotlib.image.imsave('image1.png', image2)
```
```
extractedText = pytesseract.image_to_string("image1.png",lang='eng+tam+hin+kan+tel+mal',config='--psm 6') #convert the extracted to string
extractedText= extractedText.replace('\n', ' ')
```
```
print(extractedText)
```
LINGUANAUT I'm learning Spanish right now. because | think it's a beautiful language. and also because | want to visit Spain one day. I'm improving day after day. but | need to practice with someone who Is a native. Siw MUIR CTU meroli) 

### **Language detection and making the text more understandable**
```
!pip install langdetect #detect the language 
```
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting langdetect
  Downloading langdetect-1.0.9.tar.gz (981 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 981.5/981.5 kB 48.5 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from langdetect) (1.16.0)
Building wheels for collected packages: langdetect
  Building wheel for langdetect (setup.py) ... done
  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993243 sha256=6317567ae726b42b3aff98f90445cbb2bea0f08c905cc910c385290b35a4a815
  Stored in directory: /root/.cache/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb
Successfully built langdetect
Installing collected packages: langdetect
Successfully installed langdetect-1.0.9
```
from langdetect import detect
lang = detect(extractedText)
print(lang)
```
en
```
pip install translate #to translate text from one language to another
```
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting translate
  Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)
Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from translate) (4.9.2)
Collecting libretranslatepy==2.1.1
  Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)
Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from translate) (8.1.3)
Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from translate) (2.27.1)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->translate) (3.4)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->translate) (2022.12.7)
Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->translate) (2.0.12)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->translate) (1.26.15)
Installing collected packages: libretranslatepy, translate
Successfully installed libretranslatepy-2.1.1 translate-3.6.1
```
pip install gTTs #Google Text-to-Speech
```
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting gTTs
  Downloading gTTS-2.3.1-py3-none-any.whl (28 kB)
Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.9/dist-packages (from gTTs) (2.27.1)
Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.9/dist-packages (from gTTs) (8.1.3)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27->gTTs) (2022.12.7)
Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27->gTTs) (2.0.12)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27->gTTs) (1.26.15)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27->gTTs) (3.4)
Installing collected packages: gTTs
Successfully installed gTTs-2.3.1
```
pip install gtts
```

Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: gtts in /usr/local/lib/python3.9/dist-packages (2.3.1)
Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.9/dist-packages (from gtts) (8.1.3)
Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.9/dist-packages (from gtts) (2.27.1)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27->gtts) (3.4)
Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27->gtts) (2.0.12)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27->gtts) (1.26.15)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27->gtts) (2022.12.7)

```
import os
import gtts as gt

from translate import Translator
translator= Translator(from_lang=lang,to_lang="en")
translation = translator.translate(extractedText)
print(translation)
```
LINGUANAUT I'm learning Spanish right now. because | think it's a beautiful language. and also because | want to visit Spain one day. I'm improving day after day. but | need to practice with someone who Is a native. Siw MUIR CTU meroli) 
```
trans = ""
i=0
preele= ""
for element in translation:
    i+=1
    n = ord(element)
    if 97 <= n <= 122 or 65<=n<=90 or 48<=n<=57:
        trans=trans+element
    else:
        if(preele==" "):
            trans=trans+""
        else:
            trans=trans+" "
    preele=element
      #print(element, end=' ')
    if(i==499):
        break;
print(trans)
```
LINGUANAUT I'm learning Spanish right now. because | think it's a beautiful language. and also because | want to visit Spain one day. I'm improving day after day. but | need to practice with someone who Is a native. Siw MUIR CTU meroli) 
```
trans1 = ""
i=0
preele= ""
for element in trans:
    i+=1
    n = ord(element)
    if 97 <= n <= 122 or 65<=n<=90 or 48<=n<=57:
        trans1=trans1+element
    else:
        if(preele==" "):
            trans1=trans1+""
        else:
            trans1=trans1+" "
    preele=element
      #print(element, end=' ')
    if(i==499):
        break;
print(trans1)
```
LINGUANAUT I m learning Spanish right now because think it s a beautiful language and also because want to visit Spain one day I m improving day after day but need to practice with someone who Is a native Siw MUIR CTU meroli 
```
from textblob import TextBlob
tb_txt = TextBlob(trans1)
```
```
correctedTBText = tb_txt.correct()
correctedText = str(correctedTBText)
```
```
print(str(correctedText)) #finale text after modifications 
```
LINGUANAUT I m learning Spanish right now because think it s a beautiful language and also because want to visit Pain one day I m improving day after day but need to practice with someone who Is a native In MUIR CTU merely

### **Converting the english text to tamil**
```
!pip install git+https://github.com/huggingface/transformers -q  #importing the Dataset from hugging face 
!pip install transformers -U -q  #Providies pretrained models and tokenzing tools 
!pip install sentencepiece #open-source text tokenizer and detokenizer developed by Google
!pip freeze | grep transformers #filters the output 
```
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 94.1 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.1/200.1 kB 26.6 MB/s eta 0:00:00
  Building wheel for transformers (pyproject.toml) ... done
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting sentencepiece
  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 47.5 MB/s eta 0:00:00
Installing collected packages: sentencepiece
Successfully installed sentencepiece-0.1.98
transformers @ git+https://github.com/huggingface/transformers@c8df3900c8f1ffa874fcbb528fa253496462bcc1
<br/>

```from transformers import MBartForConditionalGeneration, MBart50TokenizerFast # they are pretrained models used for convertion one language to another ```

```
model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-one-to-many-mmt") #downloading the pretrained model 
tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-one-to-many-mmt", src_lang="en_XX")
```
<img width="610" alt="image" src="https://user-images.githubusercontent.com/110376310/232096152-2c38a233-522a-4c0a-ad8e-6872366dc3b9.png">

```
model_inputs = tokenizer(correctedText, return_tensors="pt") #tokenizing the input 
```

```
generated_tokens = model.generate(
    **model_inputs,
    forced_bos_token_id=tokenizer.lang_code_to_id["ta_IN"] #setting the language 
)
```

```
translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

```
print(translation)

```
['லிங்கனவுட் நான் இப்பொழுது ஸ்பெயின் மொழியை கற்றுக் கொண்டிருக்கிறேன், ஏனெனில் இது ஒரு அழகிய மொழி என்று கருதுகிறேன், மேலும் ஒரு நாள் வலியை பார்க்க விரும்புகிறேன், நான் நாள்தோறும் மேம்பட்டு வருகிறேன், ஆனால் ஒருவருடன் பயிற்சி செய்ய வேண்டியது அவசியம்.']

```
!pip install playsound #library to play sound 

```
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting playsound
  Downloading playsound-1.3.0.tar.gz (7.7 kB)
  Preparing metadata (setup.py) ... done
Building wheels for collected packages: playsound
  Building wheel for playsound (setup.py) ... done
  Created wheel for playsound: filename=playsound-1.3.0-py3-none-any.whl size=7035 sha256=71394c05f360caaa18ef595a9746a5893c55af5725d29ed1f4d91b3162424475
  Stored in directory: /root/.cache/pip/wheels/ba/39/54/c8f7ff9a88a644d3c58b4dec802d90b79a2e0fb2a6b884bf82
Successfully built playsound
Installing collected packages: playsound
Successfully installed playsound-1.3.0

```
!pip install pydub #library for dubbing 

```
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting pydub
  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)
Installing collected packages: pydub
Successfully installed pydub-0.25.1

```
pip install ffmpeg #library for converting to speech 

```

Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting ffmpeg
  Downloading ffmpeg-1.4.tar.gz (5.1 kB)
  Preparing metadata (setup.py) ... done
Building wheels for collected packages: ffmpeg
  Building wheel for ffmpeg (setup.py) ... done
  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=9c7b0868d046d7563eebdabed0a6c4653c12afd60b4bdaf6cedbfb8bafb950af
  Stored in directory: /root/.cache/pip/wheels/1d/57/24/4eff6a03a9ea0e647568e8a5a0546cdf957e3cf005372c0245
Successfully built ffmpeg
Installing collected packages: ffmpeg
Successfully installed ffmpeg-1.4

```
txt=' '.join(translation) #convert the list to string
tts=gt.gTTS(text=txt,lang="ta") 
tts.save("ttso.wav") #speech generated 
os.system("ttso.wav")

```

```

from IPython.display import Audio, display
sound_file = 'ttso.wav'
display(Audio(sound_file, autoplay=True)) #output 

```

### **RESULT AND FINDINGS 

#### **Image text extraction: 

The use of Tesseract for text extraction from images proved to be effective in extracting text accurately from images after applying image processing techniques such as error correction, dilation, and binary conversion. However, the accuracy of text extraction may vary depending on the quality of the input image, font size, and text orientation. Further experimentation and optimization may be needed to improve the accuracy of text extraction, especially for challenging image conditions.

#### **Language detection: 

The LangDetect library was used for language detection, which accurately detected the language of the extracted text. However, it may face challenges in accurately detecting the language of short or mixed-language texts. Further evaluation and customization of the language detection model may be necessary to improve its accuracy, especially for detecting Indian languages, which may have similarities in script and vocabulary.

#### ** Text translation:

The use of mBART for text translation proved to be effective in generating translations of the extracted text into other Indian languages. The pretrained mBART model, being a state-of-the-art NLP model, demonstrated good translation quality, but it may not be perfect and may have limitations in handling certain language nuances or rare words. Fine-tuning the mBART model on domain-specific data or using other domain-specific translation models may be explored to further improve translation accuracy.

####  **Tokenization:

The tokenization feature provided by the transformers library was used to convert the input text into a format that can be fed into the mBART model. It effectively divided the text into smaller units called tokens, which are the input units for NLP models. However, tokenization may also impact the translation accuracy as it may split words or phrases in a way that affects the meaning. Careful consideration of tokenization strategies and their impact on translation accuracy is essential.

#### **Speech synthesis:

The use of a speech synthesis library to convert the translated text into speech provided an audible output in the target language, which can be useful for users who prefer spoken output or have visual impairments. However, the quality and naturalness of the synthesized speech may vary depending on the speech synthesis library and the target language. Experimentation and evaluation of different speech synthesis techniques and libraries may be needed to achieve optimal results.

#### **Ethical considerations: 

During the development and deployment of the system, ethical considerations should be taken into account, such as data privacy, data security, and bias in NLP models. Proper handling and protection of user data, ensuring fairness and transparency in text translation, and addressing potential biases in the system are important considerations to ensure responsible and ethical use of the technology.





# **Thank you!**

